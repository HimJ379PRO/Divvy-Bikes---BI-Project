{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import the packages and modules\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.wunderground.com/history/monthly/us/il/chicago/KMDW/date/2018-1'\n",
    "\n",
    "# Fetch the HTML content\n",
    "page = requests.get(base_url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Check the HTML\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We got into a problem. There are no tables in the downloaded HTML. The tables seem to be populated only when the JavaScript is run. Sadly, it seems that the package 'requests' does not run the JavaScript.\n",
    "\n",
    "## I decided to use an alternative package that suits the purpose: 'Selenium' as it uses Google Chrome to download the complete webpage and runs the JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import the packages and modules\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run headless Chrome to avoid opening a new window everytime the webpage is accessed\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Initialize a Chrome webdriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL template to scrape multiple pages\n",
    "base_url_2018 = 'https://www.wunderground.com/history/monthly/us/il/chicago/KMDW/date/2018-{page}'\n",
    "base_url_2019 = 'https://www.wunderground.com/history/monthly/us/il/chicago/KMDW/date/2019-{page}'\n",
    "\n",
    "# Create an empty list to store DataFrames from each page\n",
    "all_dfs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pages 1 to 12 for 2018 data\n",
    "for page_num in range(1, 13):\n",
    "    # Generate URL for the current page\n",
    "    url = base_url_2018.format(page=page_num)\n",
    "    \n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait until the table is present on the page\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'table[aria-labelledby=\"Days data\"]'))\n",
    "    )\n",
    "    \n",
    "    # Get page source\n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find all tables with aria-labelledby=\"Days data\"\n",
    "    tables = soup.find_all('table', attrs={'aria-labelledby': 'Days data'})\n",
    "    \n",
    "    # Check if tables were found\n",
    "    if tables:\n",
    "        # Create a dictionary to store data from each table\n",
    "        table_data = {}\n",
    "    \n",
    "        # Extract data from each table and store it in the dictionary\n",
    "        for i, table in enumerate(tables):\n",
    "            table_name = f'Table_{i+1}'\n",
    "            table_data[table_name] = []\n",
    "            \n",
    "            # Extract data rows\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                table_data[table_name].append(row_data)\n",
    "    \n",
    "        # Create pandas DataFrame from the dictionary\n",
    "        dfs = []\n",
    "        for table_name, data in table_data.items():\n",
    "            df = pd.DataFrame(data)\n",
    "            dfs.append(df)\n",
    "    \n",
    "        # Concatenate DataFrames along axis 1 to combine them into a single DataFrame\n",
    "        combined_df = pd.concat(dfs, axis=1)\n",
    "        \n",
    "        # Append the DataFrame to the list of DataFrames\n",
    "        all_dfs.append(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pages 1 to 12 for 2019 data\n",
    "for page_num in range(1, 13):\n",
    "    # Generate URL for the current page\n",
    "    url = base_url_2019.format(page=page_num)\n",
    "    \n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait until the table is present on the page\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'table[aria-labelledby=\"Days data\"]'))\n",
    "    )\n",
    "    \n",
    "    # Get page source\n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find all tables with aria-labelledby=\"Days data\"\n",
    "    tables = soup.find_all('table', attrs={'aria-labelledby': 'Days data'})\n",
    "    \n",
    "    # Check if tables were found\n",
    "    if tables:\n",
    "        # Create a dictionary to store data from each table\n",
    "        table_data = {}\n",
    "    \n",
    "        # Extract data from each table and store it in the dictionary\n",
    "        for i, table in enumerate(tables):\n",
    "            table_name = f'Table_{i+1}'\n",
    "            table_data[table_name] = []\n",
    "            \n",
    "            # Extract data rows\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                table_data[table_name].append(row_data)\n",
    "    \n",
    "        # Create pandas DataFrame from the dictionary\n",
    "        dfs = []\n",
    "        for table_name, data in table_data.items():\n",
    "            df = pd.DataFrame(data)\n",
    "            dfs.append(df)\n",
    "    \n",
    "        # Concatenate DataFrames along axis 1 to combine them into a single DataFrame\n",
    "        combined_df = pd.concat(dfs, axis=1)\n",
    "        \n",
    "        # Append the DataFrame to the list of DataFrames\n",
    "        all_dfs.append(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the pandas dataframes and save it to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames from different pages along axis 0\n",
    "final_df = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV file without the index\n",
    "final_df.to_csv('WeatherData_2018_2019_UNPROCESSED.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
